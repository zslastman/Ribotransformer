Transformer notes
AHA - of course this can't work - I'm operating directly on the input data in the integer space of words- but the model has no access to this, it only learns based on the transformation from 10k words space to 200k vector space.
autoencoder notes

a) 1d conv from n_bp to n_bp x windowsize
b) another 1d conv from n_bp x windowsize to get total cut site prob per cut site per origin site
c) then multiply by weights per origin position
d) 
Then another 1d conv, using weights from b to get a normalized and finally 1d conv to get down to a.


n
vector of cut probs per site
n
vector of 3' cut probs per site
n/
vector of ribo occ per site
n,w vector of n codon emitting 5' sites at position n-i
n,w vector of n codon emitting 3' sites at position n-i



so for each position x


d[x,l] is our footprint 5' end density for position x and length l
O[i] is our ribosome occupancy vector for position i
fp[i,w] is the probability that given a read at i, the 5' end is w bp upstream
tp[i,w] is the probability that given a read at i, the 3' end is w bp downstream

for x in range(0,n):
	for l in range(0:w):
		for i in range(0:l):
			d[x,l] = O[x+i]+fp[x+i,i]+tp[x+w,l-i]


we make tp[n,w] by first calculating FP[n], based on some sequence model, then getting fp[n,w]

with 

fp[n,w] = FP[n] + FPd[w]

where FP[n] is just how cutable that site is, and FPd is how likely the footprint is to be w bases upstream of the codon it's coming from.

Then we normalize so that

where log(fp[n,:]).sum() == 1


w[l,]

